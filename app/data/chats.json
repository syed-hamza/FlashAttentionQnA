[{"id": 1, "messages": [{"sender": "user", "text": "Thank you for your attention.\n"}, {"sender": "bot", "text": ""}, {"sender": "user", "text": "Tell me about flash attention\n"}, {"sender": "bot", "text": "The first sentence summarizes the main challenge of training large-scale machine learning models, which is the instability that can occur due to the size and complexity of the workloads. The second sentence explains that numeric deviation has emerged as a potential cause of this instability, and that it is difficult to quantify the effects of numeric deviation due to the costly nature of training runs. The third sentence introduces the framework developed in the paper to understand the effects of numeric deviation and construct proxies to put observations into context when downstream effects are difficult to quantify. The fourth sentence provides a case study of the widely-adopted Flash Attention optimization, which shows that Flash Attention sees roughly an order of magnitude more numeric deviation as compared to Baseline Attention at BF16 when measured during an isolated forward pass. The fifth sentence uses a data-driven analysis based on the Wasserstein Distance to provide upper bounds on how this numeric deviation impacts model weights during training, finding that the numerical deviation present in Flash Attention is 2-5 times less significant than low-precision training."}]}]