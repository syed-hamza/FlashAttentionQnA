[{"id": 1, "messages": [{"sender": "user", "text": "Flash attention\n"}, {"sender": "bot", "text": "  Training large-scale machine learning models poses distinct system\nchallenges, given both the size and complexity of today's workloads. Recently,\nmany organizations training state-of-the-art Generative AI models have reported\ncases of instability during training, often taking the form of loss spikes.\nNumeric deviation has emerged as a potential cause of this training\ninstability, although quantifying this is especially challenging given the\ncostly nature of training runs. In this work, we develop a principled approach\nto understanding the effects of numeric deviation, and construct proxies to put\nobservations into context when downstream effects are difficult to quantify. As\na case study, we apply this framework to analyze the widely-adopted Flash\nAttention optimization. We find that Flash Attention sees roughly an order of\nmagnitude more numeric deviation as compared to Baseline Attention at BF16 when\nmeasured during an isolated forward pass. We then use a data-driven analysis\nbased on the Wasserstein Distance to provide upper bounds on how this numeric\ndeviation impacts model weights during training, finding that the numerical\ndeviation present in Flash Attention is 2-5 times less significant than\nlow-precision training.\n"}]}]